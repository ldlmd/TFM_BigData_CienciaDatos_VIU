# -*- coding: utf-8 -*-
"""PlantillaTFM - LorenadelaMadrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4uhSsi_lTDISSwzWPkrAEY_JP9G3CU0

# <center>PLANTILLA: APRENDIZAJE NO SUPERVISADO Web Scraping eBay<center>

**Nombre y apellidos: Lorena de la Madrid Descalzo**

**Usuario VIU:**

---
# Contexto
---

<font color="green">Describir el analisis, exploración del dataset muy brevemente*</font>

---
# Inicialización e imports
---
"""

# Imports generales
import pandas as pd
import numpy as np
import io
from google.colab import files


# Semilla aleatoria arbitraria y constante a incluir en los algoritmos estocásticos para que los experimentos sean siempre reproducibles y generen los mismos resultados.
seed = 42

# Funcion de carga de ficheros .csv

def upload_files (index_fields=None):
  uploaded = files.upload()
  for fn in uploaded.keys():
    print('User uploaded file "{name}" with length {length} bytes'.format(
        name=fn, length=len(uploaded[fn])))
    df = pd.read_csv(io.StringIO(uploaded[fn].decode('utf-8')), index_col = index_fields)
    return df

# Imports especificos

from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score, silhouette_score
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import DBSCAN
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import davies_bouldin_score, make_scorer

# Cargar el conjunto de datos originales resultado de webscraping
dataset0 = upload_files()
print(dataset0.shape)
dataset0.head()

# transformar a minusculas todo el dataset

dataset0 = dataset0.applymap(lambda s: s.lower() if type(s) == str else s)
dataset0.head()

dataset0.info()

"""---
# Exploración preliminar y Primeras transformaciones
---

En este apartado se incluyen las transformaciones básicas sobre el dataset para generar un dataset con la informacion relevante para el objetivo: Aplicar tecnicas de aprendizaje No Supervisado.

## Eliminacion de columnas
"""

# Eliminar columnas innecesarias : URL_de_página_de_detalles y fecha_y_hora(webscraping)
dataset0.drop('URL_de_página_de_detalles', axis = 1, inplace = True)
dataset0.drop('Fecha_y_hora', axis = 1, inplace = True)
dataset0.head()

# contar el numero de filas que hay por valor del campo palabra_clave

counts = dataset0.groupby('Palabra_clave').size()
print(counts)

# Ver la distribucion por tipo de producto: plabra_clave

from matplotlib import pyplot as plt
import seaborn as sns
dataset0.groupby('Palabra_clave').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# eliminar las filas con palabra_clave = auriculares inalambricos

dataset0 = dataset0[~dataset0['Palabra_clave'].isin(['auriculares inalambricos'])]

counts = dataset0.groupby('Palabra_clave').size()
print(counts)

dataset0.shape

"""Tratamiento de los falsos positivos en el proceso de webscraping"""

# Identificar las filas que para cada palabra clave, contengan alguna de las palabras identificadas como falso positivo .
#Contar el total de filas y eliminarlas del dataset

# Filtrar las filas que corresponden a cada tipo de producto
df1 = dataset0[dataset0['Palabra_clave'] == 'auriculares']
df2 = dataset0[dataset0['Palabra_clave'] == 'webcam']
df3 = dataset0[dataset0['Palabra_clave'] == 'raton']
df4 = dataset0[dataset0['Palabra_clave'] == 'power bank']
df5 = dataset0[dataset0['Palabra_clave'] == 'altavoces']
df6 = dataset0[dataset0['Palabra_clave'] == 'cargador']

# Crear una máscara booleana para identificar las filas que contienen alguna de las palabras clave
mask1 = df1['Producto'].str.contains('almohadilla|funda|cojines|diadema', case=False, na=False)
mask2 = df2['Producto'].str.contains('tapa|placa|cubierta|cover|sticker|protector|funda|asus', case=False, na=False)
mask3 = df3['Producto'].str.contains('alfombrilla|rueda|wheel|bateria|reparacion', case=False, na=False)
mask4 = df4['Producto'].str.contains('solar|estuche|caja', case=False, na=False)


# Contar el número de filas que cumplen la condición
num_filas_eliminar1 = mask1.sum()
print(f"Número de filas a eliminar de auriculares: {num_filas_eliminar1}")

num_filas_eliminar2 = mask2.sum()
print(f"Número de filas a eliminar de webcam: {num_filas_eliminar2}")

num_filas_eliminar3 = mask3.sum()
print(f"Número de filas a eliminar de raton: {num_filas_eliminar3}")

num_filas_eliminar4 = mask4.sum()
print(f"Número de filas a eliminar de power bank: {num_filas_eliminar4}")

# Eliminar las filas que cumplen la condición del dataset original
dataset0 = dataset0.drop(df1[mask1].index)
dataset0 = dataset0.drop(df2[mask2].index)
dataset0 = dataset0.drop(df3[mask3].index)
dataset0 = dataset0.drop(df4[mask4].index)
#dataset0 = dataset0.drop(df5[mask5].index)

# Mostrar el dataset actualizado
dataset0.head()
dataset0.shape

counts = dataset0.groupby('Palabra_clave').size()
print(counts)

"""**Crear nuevas caracteristicas a traves de la columna producto (descripcion del producto)**"""

# Crear una nueva columna: marca, que extraiga el valor del campo producto si contiene una palabra de un array

marcas = ['andersson','lenovo','jbl','jabra','creative','corsair','sony','amazon','pioner','amazfit','android','blackview','sennheiser','philips','bose','oppo','logitech', 'samsung', 'huawei', 'xiaomi', 'apple','silvercrest','sony','asus','nintendo','dell','canon','acer']

def extraer_marca(producto):
  for marca in marcas:
    if marca in producto:
      return marca
  return 'otra'


dataset0['Marca'] = dataset0['Producto'].apply(extraer_marca)

dataset0.head()

# contar filas por valor del atributo marca

counts_marca = dataset0.groupby('Marca').size()
print(counts_marca)

# Eliminar columna producto tras extraer el conocimiento necesario de ella:
dataset0.drop('Producto', axis = 1, inplace = True)
dataset0.head()

# descargar en csv el dataset con las primeras transformaciones

dataset0.to_csv('dataset_transformado.csv', encoding='utf-8')
files.download('dataset_transformado.csv')

"""## Transformaciones de texto

**Transformaciones en el atributo precio**
"""

# Eliminar EUR de los valores de la columna precio

dataset0['Precio'] = dataset0['Precio'].str.replace('eur', '', regex=False)

# Función para extraer los valores max y min del precio
def extraer_valores_precio(precio):

    precio = precio.strip().replace('.', '').replace(',', '.')

    if ' a ' in precio:
        precio_min = float(precio.split(' a ')[0])  # Extrae el valor antes de 'a'
        precio_max = float(precio.split(' a ')[1])     # Extrae el valor después de 'a'
        return precio_min, precio_max
    else:
        return float(precio), float(precio)  # Mantiene el valor original en ambas columnas

# Aplicar la función y crear nuevas columnas
dataset0[['precio_min', 'precio_max']] = dataset0['Precio'].apply(extraer_valores_precio).apply(pd.Series)

# Calcular el valor medio
dataset0['precio_medio'] = ((dataset0['precio_min'] + dataset0['precio_max']) / 2).round().astype(int)

# Eliminar columnas innecesarias tras la generacion del atributo precio_medio
dataset0.drop('Precio', axis = 1, inplace = True)
dataset0.drop('precio_min', axis = 1, inplace = True)
dataset0.drop('precio_max', axis = 1, inplace = True)

"""**Transformaciones del atributo precio_envio**"""

# Eliminar palabras innecesarias de los valores de la columna precio_envio
dataset0['Precio_Envío'] = dataset0['Precio_Envío'].str.replace(r'envío|en|eur|de|no|especificado|estimado|\+', '', regex=True)

# Transformacion para float
dataset0['Precio_Envío'] = dataset0['Precio_Envío'].str.replace(',', '.')

def validar_y_limpiar_precio(precio_envio):
    if isinstance(precio_envio, str):  # Check if the value is a string before using string methods
        if precio_envio == ' gratis':
            return 0  # 0 para representar gratis
        elif precio_envio.strip() == '':
            return float('nan')
        else:
            try:
                return float(precio_envio)
            except ValueError:
                return float('nan')  # para los casos de errores
    elif isinstance(precio_envio, float):
        if precio_envio == float('nan'):
            return float('nan')
        else:
            return precio_envio
    else:
        return float('nan') # Handle other data types if present by converting to NaN

# Aplicar la función a la columna precio_envio
dataset0['precio_envio_limpio'] = (dataset0['Precio_Envío']).apply(validar_y_limpiar_precio)

# Redondear y convertir a entero, manteniendo nulos
dataset0['precio_envio_int'] = dataset0['precio_envio_limpio'].apply(
    lambda x: int(round(x)) if pd.notnull(x) else pd.NA
)

# Convertir a tipo entero (Int64) para manejar NaN
dataset0['precio_envio_int'] = dataset0['precio_envio_int'].astype('Int64')

"""Transformaciones variable localidad"""

# Eliminar "desde" de los valores de la columna Localidad

dataset0['Localidad'] = dataset0['Localidad'].str.replace('desde ', '', regex=False)

# Eliminar columnas innecesarias tras la generacion del atributo precio_medio
dataset0.drop('Precio_Envío', axis = 1, inplace = True)
dataset0.drop('precio_envio_limpio', axis = 1, inplace = True)

dataset0.info()

# descargar en csv el dataset con las transformaciones

dataset0.to_csv('dataset_transformado2.csv', encoding='utf-8')
files.download('dataset_transformado2.csv')

"""*Validadas las distintas transformaciones en las iteraciones de ejecución intermedias.*

# **Exploracion de los nulos**

**Exploracion general para todos los atributos**
"""

# Calcular el porcentaje de valores nulos por columna
null_percentage = dataset0.isnull().mean() * 100

# Imprimir el porcentaje de nulos por atributo
print("Porcentaje de valores nulos por atributo:")
print(null_percentage)

"""Solo existen valores nulos en los atributos, localidad y precio_envio

**Se analizan en profundidad los atributos con nulos**

Atributo: localidad
"""

cantidad_nulos = len(dataset0['Localidad']) - dataset0['Localidad'].count() # Conteo de nulos

if cantidad_nulos > 0:
    print(f"Cantidad de nulos en el atributo Localidad: {cantidad_nulos}\n") # Impresión de la cantidad de nulos
else:
    print("No existen filas con valores nulos para este atributo.\n")

# Calcular el porcentaje de valores nulos por localidad
null_percentage_by_localidad = dataset0.groupby('Localidad').apply(lambda x: x.isnull().mean() * 100)

# Imprimir el porcentaje de nulos por localidad
print("Porcentaje de valores nulos por localidad:")
print(null_percentage_by_localidad)

"""Se encuentran la mayoria para la localidad Canada y Alemania.

Atributo: precio_envio
"""

cantidad_nulos = len(dataset0['precio_envio_int']) - dataset0['precio_envio_int'].count() # Conteo de nulos

if cantidad_nulos > 0:
    print(f"Cantidad de nulos en el atributo precio_envio: {cantidad_nulos}\n") # Impresión de la cantidad de nulos
else:
    print("No existen filas con valores nulos para este atributo.\n")

"""**Procesamiento para los valores nulos**"""

dataset0.fillna('Desconocido', inplace=True)

"""# **Verificacion de la calidad de los datos**

En base al conocimiento experto del dominio reflejado en las tablas de la memoria

Calidad del atributo precio
"""

#Verificar que los valores del atributo se encuentren dentro de rango

# Se identifica y cuenta a los valores que no cumplen la condición definida.

resultado = dataset0[dataset0['precio_medio'] > 1500]
resultado2 = dataset0[dataset0['precio_medio'] < 9]

print("Se visualizan las filas con errores de rango:")
display(resultado) # Para visualizar las tuplas con valores nulos o erróneos
display(resultado2)
print(f"Cantidad detectada precios aparentemente altos: {resultado.shape[0]}")
print(f"Cantidad detectada precios aparentemente bajos: {resultado2.shape[0]}")

"""Calidad del atributo precio_envio"""

#Verificar que los valores del atributo se encuentren dentro de rango

# Se identifica y cuenta a los valores que no cumplen la condición definida. Esta condicion nos la podría dar el experto del dominio

resultado = dataset0[dataset0['precio_envio_int'] > 100]

print("Se visualizan las filas con errores de rango:")
display(resultado) # Para visualizar las tuplas con valores nulos o erróneos

print(f"Cantidad detectada con precios de envio aparentemente erroneos o fuera de rango: {resultado.shape[0]}")

"""# **Categorizacion de las variables numericas**

Categorizar el atributo precio
"""

# Categorizar el atribiuto precio: rangos de precios
def clasificar_precio(precio):
    if precio == 0:  # Representa "gratis"
        return 'gratis'
    elif precio >= 500:
        return 'muy caro'
    elif 51 <= precio <= 499:
        return 'caro'
    elif 21 <= precio <= 50:
        return 'medio'
    elif 20 >= precio >= 11:
        return 'barato'
    elif 10 >= precio >= 1:
        return 'muy barato'
    # Manejar los casos NaN:
    else:
        return 'desconocido'


dataset0['rango_precio'] = dataset0['precio_medio'].apply(clasificar_precio)

# Distribución de valores para la variable rango_precio
rango_precio_counts = dataset0['rango_precio'].value_counts()
print(rango_precio_counts)

# Gráfico de barras para visualizar la distribución
plt.figure(figsize=(8, 6))
sns.countplot(x='rango_precio', data=dataset0)
plt.title('Distribución de Rango de Precio')
plt.xlabel('Rango de Precio')
plt.ylabel('Frecuencia')
plt.show()

"""Categorizar el atributo precio del envío"""

# Obtener estadísticas descriptivas del atributo precio_envio_int
stats_precio_envio = dataset0['precio_envio_int'].describe()

print("Estadísticas del atributo precio_envio_int:")
print(stats_precio_envio)

# Categorizar el atribiuto precio de envio: rangos de precios
def clasificar_precio2(precio_envio):
    if precio_envio == 0:  # Representa "gratis"
        return 'gratis'
    elif precio_envio >= 100:
        return 'muy caro'
    elif 21 <= precio_envio <= 99:
        return 'caro'
    elif 5 <= precio_envio <= 20:
        return 'medio'
    elif 4 >= precio_envio >= 1:
        return 'barato'
    # Manejar los casos NaN:
    else:
        return 'desconocido'


dataset0['rango_precio_envio'] = dataset0['precio_envio_int'].apply(clasificar_precio2)

# Distribución de valores para la variable rango_precio
rango_precio_counts = dataset0['rango_precio_envio'].value_counts()
print(rango_precio_counts)

# Gráfico de barras para visualizar la distribución
plt.figure(figsize=(8, 6))
sns.countplot(x='rango_precio_envio', data=dataset0)
plt.title('Distribución de Rango de Precio de envio')
plt.xlabel('Rango de Precio Envio')
plt.ylabel('Frecuencia')
plt.show()

"""Eliminacion de los atributos intermedios"""

dataset0.head()

# Eliminar columnas innecesarias tras la generacion del atributo precio_medio
dataset0.drop('precio_envio_int', axis = 1, inplace = True)
dataset0.drop('precio_medio', axis = 1, inplace = True)

"""**RENOMBRAR ATRIBUTOS**"""

dataset0.rename(columns={'Palabra_clave': 'Tipo_producto', 'rango_precio_envio':'precio_envio','rango_precio':'precio'}, inplace=True)
dataset0.head()

# descargar en csv el dataset con las transformaciones

dataset0.to_csv('dataset_transformado_final.csv', encoding='utf-8')
files.download('dataset_transformado_final.csv')

"""# **Exploracion del dataset final**

## **Estadisticos y distribución de los atributos**

**Estadisticos atributos**
"""

# Estadísticos descriptivos para atributos numéricos
print(dataset0.describe())

"""**Histogramas: Distribucion de valores**"""

# Distribución de valores para cada atributo numérico
for column in dataset0.select_dtypes(include=np.number):
  plt.figure(figsize=(8, 6))
  sns.histplot(dataset0[column], kde=True)
  plt.title(f'Distribución de {column}')
  plt.xlabel(column)
  plt.ylabel('Frecuencia')
  plt.show()

# Distribución de valores para cada atributo categórico
for column in dataset0.select_dtypes(include=['object', 'category']):
  plt.figure(figsize=(8, 6))
  sns.countplot(x=column, data=dataset0)
  plt.title(f'Distribución de {column}')
  plt.xlabel(column)
  plt.ylabel('Frecuencia')
  plt.xticks(rotation=45, ha='right')  # Rotar las etiquetas del eje x para mejor visualización
  plt.show()

"""**Analisis de relacion entre las variables categoricas**

Analisis general para todos los pares de atributos
"""

atributos = dataset0.columns.tolist()
atributos

# Inicializar la variable de resultados
results = []

# Calcular tablas de contingencia y realizar pruebas Chi-Cuadrado
for i in range(len(atributos)):
    for j in range(i + 1, len(atributos)):
        var1 = atributos[i]
        var2 = atributos[j]

        # Crear tabla de contingencia
        contingency_table = pd.crosstab(dataset0[var1], dataset0[var2])

        # Realizar prueba Chi-Cuadrado
        chi2, p, dof, expected = chi2_contingency(contingency_table)

        # Almacenar resultados
        results.append({
            "Variable 1": var1,
            "Variable 2": var2,
            "Chi2": chi2,
            "p-valor": p,
            "Tabla de Contingencia": contingency_table
        })

# Convertir resultados a DataFrame para facilitar la visualización
results_relaciones_df = pd.DataFrame(results)

results_relaciones_df.head()

# descargar en csv los resultados del analisis de relaciones

results_relaciones_df.to_csv('dataset_analisis_relaciones.csv', encoding='utf-8')
files.download('dataset_analisis_relaciones.csv')

"""Ánalisis de los resultados y aplicar valores significativos"""

# Filtrar relaciones significativas (Se selecciona: p-valor < 0.05)
significant_relationships = results_relaciones_df[results_relaciones_df['p-valor'] < 0.05]

# Mostrar resultados significativos
print("Relaciones significativas entre pares de variables:")
print(significant_relationships[['Variable 1', 'Variable 2', 'Chi2', 'p-valor']])

# Mostrar tablas de contingencia para relaciones significativas
#for index, row in significant_relationships.iterrows():
    #print(f"\nTabla de Contingencia entre {row['Variable 1']} y {row['Variable 2']}:")
    #print(row['Tabla de Contingencia'])

"""Todas las relaciones son significativas!"""

import seaborn as sns

for i in range(len(atributos)):
    for j in range(i + 1, len(atributos)):
        var1 = atributos[i]
        var2 = atributos[j]

        contingency_table = pd.crosstab(dataset0[var1], dataset0[var2])
        sns.heatmap(contingency_table, annot=True, cmap='YlGnBu')
        plt.title('Mapa de Calor')
        plt.show()

import matplotlib.pyplot as plt

for i in range(len(atributos)):
    for j in range(i + 1, len(atributos)):
        var1 = atributos[i]
        var2 = atributos[j]

        contingency_table = pd.crosstab(dataset0[var1], dataset0[var2])
        contingency_table.plot(kind='bar')
        plt.title('Relacion entre variables')
        plt.xlabel(var1)
        plt.ylabel(var2)
        plt.show()

"""---
# Clustering
---

Para las pruebas sucesivas del modelo, se carga directamente el dataset ya final tras las transformaciones anteriores, para ahorrar tiempo en la ejecucion
"""

dataset0 = upload_files()
print(dataset0.shape)
dataset0.head()

column_names = dataset0.columns.tolist()

print("Columnas del dataset:")
for column in column_names:
  print(column)

dataset0.drop('Unnamed: 0', axis = 1, inplace = True)
dataset0.info()

"""## **K-Modes**"""

# instalar kmodes

!pip install kmodes

import matplotlib.pyplot as plt
from kmodes.kmodes import KModes

sse = []
k_values = range(1, 5)

for k in k_values:
    km = KModes(n_clusters=k, init='Huang', n_init=5)
    km.fit_predict(dataset0)
    sse.append(km.cost_)

plt.plot(k_values, sse, marker='o')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Suma de Errores Cuadráticos (SSE): Coste')
plt.title('Método del Codo')
plt.show()

"""**Construccion del modelo para distintas configuraciones**

KModes (k=3, init=Huang)
"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 3  # Número de clusters
init = 'Huang'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo

km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k3_huang = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k3_huang['cluster'] = labels
df_kmeans_k3_huang['cluster'].value_counts()

"""KModes (k=3, init=Cao)"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 3  # Número de clusters
init = 'Cao'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo


km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k3_cao = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k3_cao['cluster'] = labels
df_kmeans_k3_cao['cluster'].value_counts()

"""KModes (k=2, init=Huang)"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 2  # Número de clusters
init = 'Huang'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo


km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k2_huang = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k2_huang['cluster'] = labels
df_kmeans_k2_huang['cluster'].value_counts()

"""KModes (k=2, init=Cao)"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 2  # Número de clusters
init = 'Cao'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo


km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k2_cao = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k2_cao['cluster'] = labels
df_kmeans_k2_cao['cluster'].value_counts()

"""### **Evaluacion y pruebas de modelos**

**MLflow en Visual Studio**

Esta parte de evaluacion del modelos para distintas configuraciones de los aprametros (k, init) del algoritmo kmodes se lleva a cabo en el IDE: Visual Studio

**Evaluacion y analisis manual**
"""

#Contar la cantidad de elementos en cada cluster:

cluster_counts = df_kmeans_k3_huang['cluster'].value_counts()
print(cluster_counts)


#Ver las características de cada cluster

# Agrupar por cluster y ver las características más comunes
grouped = df_kmeans_k3_huang.groupby('cluster').agg(lambda x: x.mode()[0])
print(grouped)

"""**Análisis visual del clustering**"""

!pip install prince

df_kmeans_k2_huang.head()

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df_kmeans_k3_huang.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df_kmeans_k3_huang.index)
mca_df['cluster'] = df_kmeans_k3_huang['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=3, init=Huang) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

#Codificación de Variables Categóricas para Davies-Bouldin-Score
# Extract features and labels
X = df_kmeans_k3_huang.drop('cluster', axis=1)
labels = df_kmeans_k3_huang['cluster']

encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(X)


#Calcular el Davies-Bouldin-Score
davies_bouldin = davies_bouldin_score(X_encoded, labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df_kmeans_k3_cao.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df_kmeans_k3_cao.index)
mca_df['cluster'] = df_kmeans_k3_cao['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=3, init=Cao) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df_kmeans_k2_huang.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df_kmeans_k2_huang.index)
mca_df['cluster'] = df_kmeans_k2_huang['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=2, init=Huang) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

#Codificación de Variables Categóricas para Davies-Bouldin-Score
# Extract features and labels
X = df_kmeans_k2_huang.drop('cluster', axis=1)
labels = df_kmeans_k2_huang['cluster']

encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(X)


#Calcular el Davies-Bouldin-Score
davies_bouldin = davies_bouldin_score(X_encoded, labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df_kmeans_k2_cao.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df_kmeans_k2_cao.index)
mca_df['cluster'] = df_kmeans_k2_cao['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=2, init=Cao) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

"""**Visualizacion sin MCA**"""

# Representacion grafica por cada atributo segun el cluster al que pertenecen

# df_kmeans_k3_huang
for column in df_kmeans_k3_huang.columns:
  if column != 'cluster':
    plt.figure(figsize=(10, 6))
    sns.countplot(x=column, hue='cluster', data=df_kmeans_k3_huang)
    plt.title(f'Distribución de {column} por Cluster')
    plt.xlabel(column)
    plt.ylabel('Frecuencia')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility
    plt.show()

"""**No se incluye el atributo marca.**"""

dataset0.drop('Marca', axis=1, inplace=True)

"""k=2 init='Huang'"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 2  # Número de clusters
init = 'Huang'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo


km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k2_huang = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k2_huang['cluster'] = labels
df_kmeans_k2_huang['cluster'].value_counts()

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df_kmeans_k2_huang.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df_kmeans_k2_huang.index)
mca_df['cluster'] = df_kmeans_k2_huang['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=2, init=Huang) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

"""Metrica: davies bouldin score"""

!pip install scikit-learn

"""con el metodo de encoder: dummies"""

from sklearn.metrics import davies_bouldin_score


# Extract features and labels
X = df_kmeans_k2_huang.drop('cluster', axis=1)
labels = df_kmeans_k2_huang['cluster']


# Convert categorical features to numerical representations for Davies-Bouldin score
X_encoded = pd.get_dummies(X)


# Calculate Davies-Bouldin score
davies_bouldin = davies_bouldin_score(X_encoded, labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

"""Con el metodo de encoder: OneHotEncoder"""

from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score

#Codificación de Variables Categóricas

X = df_kmeans_k2_huang.drop('cluster', axis=1)
labels = df_kmeans_k2_huang['cluster']
encoder = OneHotEncoder(sparse_output=False)
X_encoded2 = encoder.fit_transform(X)


#Calcular el Davies-Bouldin-Score
davies_bouldin2 = davies_bouldin_score(X_encoded2, labels)
print(f"Davies-Bouldin Index: {davies_bouldin2}")

"""k=3 init='Huang'"""

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 3  # Número de clusters
init = 'Huang'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo


km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset0)

# Se preserva el conjunto de datos original para evitar problemas
df_kmeans_k3_huang = dataset0.copy()

# Se agregan las labels generadas a traves de kmodes
df_kmeans_k3_huang['cluster'] = labels
df_kmeans_k3_huang['cluster'].value_counts()

"""metrica de evaluacion"""

#Codificación de Variables Categóricas

X = df_kmeans_k3_huang.drop('cluster', axis=1)
labels = df_kmeans_k3_huang['cluster']

encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(X)


#Calcular el Davies-Bouldin-Score
davies_bouldin = davies_bouldin_score(X_encoded, labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

"""## **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**

En base a los resultados obtenidos en el clustering anterior, se decide eliminar el atributo marca.
"""

dataset0.drop('Marca', axis=1, inplace=True)

pip install prince

"""Se prueban con distitnas medidas de distancias: hamming, jaccard"""

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from prince import MCA
import matplotlib.pyplot as plt
import prince

encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(dataset0)

# escalado de los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# DBSCAN
dbscan = DBSCAN(metric='jaccard',eps=0.1, min_samples=20)
labels = dbscan.fit_predict(X_scaled)


dataset0['dbscan_cluster'] = labels

# Resultados
print(dataset0['dbscan_cluster'].value_counts())

# Visualizacion(MCA)

mca = prince.MCA(n_components=2)
mca_results = mca.fit_transform(dataset0.drop('dbscan_cluster', axis=1))

# crear df para los resultados
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=dataset0.index)
mca_df['cluster'] = dataset0['dbscan_cluster']

# Visualziacion
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering DBSCAN usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

dataset0.head()

# eliminar dbscan_cluster

dataset0.drop('dbscan_cluster', axis=1, inplace=True)
dataset0.head()

"""Cuando se obtiene solo 1 cluster no se puede usar DBS"""

# Caracteristicas asociadas a cada cluster
grouped = dataset0.groupby('dbscan_cluster').agg(lambda x: x.mode()[0])
print("\nCharacteristics of each cluster:\n", grouped)

"""Seria la moda de cada atributo categorico"""

from google.colab import files
grouped.to_csv('dataset_clustering_DBSCAN.csv', encoding='utf-8')
files.download('dataset_clustering_DBSCAN.csv')

"""Optimizacion de hiperparametros"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import DBSCAN
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import davies_bouldin_score, make_scorer


# Transformar variables categóricas a numéricas
encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset0) # Sin atributo marca

# Definir los parámetros para la búsqueda
param_grid = {
    'eps': np.arange(0.1, 1.0, 0.2),
    'min_samples': range(12, 15)
}

# Crear un scorer para el Davies-Bouldin Score
dbs_scorer = make_scorer(davies_bouldin_score)

# DBSCAN como modelo
dbscan = DBSCAN(metric='hamming')

# Grid Search
grid_search = GridSearchCV(dbscan, param_grid, scoring=dbs_scorer, cv=5)
grid_search.fit(data_encoded)

print("Mejores parámetros (Grid Search):", grid_search.best_params_)
print("Mejor puntuación (Grid Search):", -grid_search.best_score_)  # Invertir la puntuación

# Random Search
random_search = RandomizedSearchCV(dbscan, param_grid, n_iter=20, scoring=dbs_scorer, cv=5, random_state=42)
random_search.fit(data_encoded)

print("Mejores parámetros (Random Search):", random_search.best_params_)
print("Mejor puntuación (Random Search):", -random_search.best_score_)  # Invertir la puntuación

"""Si se obtiene como mejor puntuacion: nan , siginifica que el numero de clusters es 1

ALTERNATIVA: REDUCIR LA CARDINALIDAD DE ATRIBUTOS
"""

print(dataset0['Localidad'].unique())

# hacer una copia del dataset0

import shutil
dataset1 = dataset0.copy()

dataset1.head()

# MaPEAR
city_to_region = {

'china' : 'asia',
'lituania' : 'europa',
'alemania' : 'europa',
'japón' : 'asia',
'estados unidos' : 'norteamerica',
'reino unido' : 'europa',
'irlanda' : 'europa',
'francia' : 'europa',
'italia' : 'europa',
'canadá' : 'norteamerica',
'hong kong' : 'asia',
'australia' : 'oceania',
'grecia' : 'europa',
'eslovaquia' : 'europa',
'polonia' : 'europa',
'sri lanka': 'asia',
'españa' : 'europa',
'rumanía' : 'europa',
'países bajos' : 'europa',
'singapur' : 'asia',
'eslovenia' : 'europa',
'croacia' : 'europa',
'malasia' : 'asia',
'emiratos árabes unidos' : 'asia',
'dinamarca' : 'europa',
'corea del sur' : 'asia',
'república checa' : 'europa',
'letonia' : 'europa',
'liechtenstein' : 'europa',
'austria' : 'europa',
'india' : 'asia',
'ucrania' : 'europa',
'kazajistán' : 'asia',
'bélgica': 'europa',
'suiza' : 'europa',
'israel' : 'asia',
'finlandia' : 'europa',
'bulgaria': 'europa',
'Desconocido' : 'desconocido',
'eslovenia' : 'europa'

}

# Crear una nueva columna para la región: Continente
dataset1['Region'] = dataset1['Localidad'].map(city_to_region)
dataset1 = dataset1.drop(columns=['Localidad'])

dataset1.head()

"""Se repiten los analis anteriores

Prueba previa sin optimizacion de hiperaparametros
"""

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from prince import MCA


encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(dataset1)

# Scale the data (important for DBSCAN)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Apply DBSCAN
dbscan = DBSCAN(metric='hamming',eps=0.1, min_samples=20) # Adjust eps and min_samples as needed
labels = dbscan.fit_predict(X_scaled)

# Add the cluster labels to your DataFrame
dataset1['dbscan_cluster'] = labels

# Analyze the results (e.g., number of clusters, noise points)
print(dataset1['dbscan_cluster'].value_counts())

# Visualize the clusters (if possible with reduced dimensions using techniques like PCA or MCA)

#Example using MCA for visualization (as in the original code)
mca = prince.MCA(n_components=2)
mca_results = mca.fit_transform(dataset1.drop('dbscan_cluster', axis=1))

# Create a DataFrame for the MCA results
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=dataset1.index)
mca_df['cluster'] = dataset1['dbscan_cluster']

# Visualize the results
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering DBSCAN usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

dataset1.head()

# eliminar dbscan_cluster

dataset1.drop('dbscan_cluster', axis=1, inplace=True)

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import DBSCAN
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import davies_bouldin_score, make_scorer


# Transformar variables categóricas a numéricas
encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset1) # Sin atributo marca

# Definir los parámetros para la búsqueda
param_grid = {
    'eps': np.arange(0.1, 1.0, 0.2),
    'min_samples': range(12, 15)
}

# Crear un scorer para el Davies-Bouldin Score
dbs_scorer = make_scorer(davies_bouldin_score)

# DBSCAN como modelo
dbscan = DBSCAN(metric='hamming')

# Grid Search
grid_search = GridSearchCV(dbscan, param_grid, scoring=dbs_scorer, cv=5)
grid_search.fit(data_encoded)

print("Mejores parámetros (Grid Search):", grid_search.best_params_)
print("Mejor puntuación (Grid Search):", -grid_search.best_score_)  # Invertir la puntuación

# Random Search
random_search = RandomizedSearchCV(dbscan, param_grid, n_iter=20, scoring=dbs_scorer, cv=5, random_state=42)
random_search.fit(data_encoded)

print("Mejores parámetros (Random Search):", random_search.best_params_)
print("Mejor puntuación (Random Search):", -random_search.best_score_)  # Invertir la puntuación

"""## **Hierarchical Clustering**"""

dataset1.head()

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import DBSCAN
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import davies_bouldin_score, make_scorer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from prince import MCA

from scipy.cluster.hierarchy import linkage, fcluster, dendrogram

"""Contruccion del modelo: se han ido haciendo distintas pruebas con el mismo codigo"""

# Construccion del modelo

encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset1)

Z = linkage(data_encoded, method='ward')
hierarchical_labels = fcluster(Z, t=5, criterion='maxclust')

# Añadimos el clustering al dataset original
dataset1['dbscan_cluster'] = hierarchical_labels
# Resultado del clusterin
print(dataset1['dbscan_cluster'].value_counts())

"""Metricas de evaluacion del modelo"""

# Davies-Bouldin y Silhouette Scores
hierarchical_db_score = davies_bouldin_score(data_encoded, hierarchical_labels)
hierarchical_silhouette_score = silhouette_score(data_encoded, hierarchical_labels)
print("Hierarchical Clustering Davies-Bouldin Score:", hierarchical_db_score)
print("Hierarchical Clustering Silhouette Score:", hierarchical_silhouette_score)

"""Representacion grafica"""

# Representacion grafica del clustering

pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_encoded)

# Visualización de Hierarchical Clustering
plt.figure(figsize=(10, 5))
sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=hierarchical_labels, palette='deep', s=100)
plt.title("Hierarchical Clustering Results (PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title='Clusters')
plt.show()

# -------------------------
# Dendrograma
# -------------------------
plt.figure(figsize=(10, 7))
dendrogram(Z)
plt.title("Dendrogram for Hierarchical Clustering")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

dataset1.head()

dataset1.drop('dbscan_cluster' , axis=1, inplace=True)

"""## **Agglomerative Clustering**"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score
from prince import MCA
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Transformar variables categóricas a numéricas usando One-Hot Encoding
encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset1)


# -------------------------
# Agglomerative Clustering
# -------------------------

# Definir el modelo
agglomerative = AgglomerativeClustering(n_clusters=3, linkage='single')

# Ajustar el modelo y predecir los clusters
agg_labels = agglomerative.fit_predict(data_encoded)

# Calcular el Silhouette Score
agg_silhouette_score = silhouette_score(data_encoded, agg_labels)

print("Agglomerative Clustering Silhouette Score:", agg_silhouette_score)

davies_bouldin = davies_bouldin_score(data_encoded, agg_labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")


pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_encoded)


# Visualización de Agglomerative Clustering
plt.figure(figsize=(10, 5))
sns.scatterplot(x=data_pca[:, 0], y=data_pca[:, 1], hue=agg_labels, palette='deep', s=100)
plt.title("Agglomerative Clustering Results (PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title='Clusters')
plt.show()

dataset1.head()

# eliminar dbscan_cluster

dataset1.drop('dbscan_cluster', axis=1, inplace=True)

# Transformar variables categóricas a numéricas usando One-Hot Encoding
encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset1)

!pip install scipy

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage # Importa dendrogram y linkage

# Dendograma

# Calcular el linkage para el dendrograma
Z = linkage(data_encoded, method='ward')

# Trazar el dendrograma
plt.figure(figsize=(10, 7))
plt.title("Dendrograma para Agglomerative Clustering con datos categóricos")
dendrogram(Z)
plt.xlabel("Puntos de datos")
plt.ylabel("Distancia")
plt.show()

"""Se realiza experiemntacion entre distintas configuraciones/parametrizaciones"""

# -------------------------
# Agglomerative Clustering
# -------------------------

# Definir el modelo
agglomerative = AgglomerativeClustering(n_clusters=5, linkage='complete')

# Ajustar el modelo y predecir los clusters
agg_labels = agglomerative.fit_predict(data_encoded)

"""Evaluacion del modelo: visual y metricas"""

# Calcular metricas

# Calcular el Silhouette Score
agg_silhouette_score = silhouette_score(data_encoded, agg_labels)
print("Agglomerative Clustering Silhouette Score:", agg_silhouette_score)

# Calcular el Davies Bouldin Score
davies_bouldin = davies_bouldin_score(data_encoded, agg_labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

# Apply MCA para la reduccion de componentes para la representacion grafica
mca = MCA(n_components=2)
mca_results = mca.fit_transform(pd.DataFrame(data_encoded)) # Convert back to DataFrame

# Create a DataFrame for the MCA results
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'])
mca_df['cluster'] = agg_labels

# Visualize the results
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Agglomerative Clustering usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

"""Optimizacion de hiperparametros"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import davies_bouldin_score, make_scorer
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Transformar variables categóricas a numéricas
encoder = OneHotEncoder(sparse_output=False)
data_encoded = encoder.fit_transform(dataset1)

# Parámetros para búsqueda
param_grid_agglomerative = {
    'n_clusters': range(2, 4),
    'linkage': ['ward', 'complete', 'average']
}

# Crear un scorer para el Davies-Bouldin Score
dbs_scorer = make_scorer(davies_bouldin_score, greater_is_better=False)

# Agglomerative Clustering
agglomerative = AgglomerativeClustering()

# Grid Search para Agglomerative Clustering
grid_search_agglomerative = GridSearchCV(agglomerative, param_grid_agglomerative, scoring=dbs_scorer, cv=5)
grid_search_agglomerative.fit(data_encoded)

print("Mejores parámetros (Agglomerative Grid Search):", grid_search_agglomerative.best_params_)
print("Mejor puntuación (Agglomerative Grid Search):", -grid_search_agglomerative.best_score_)  # Invertir la puntuación

# Random Search para Agglomerative Clustering
random_search_agglomerative = RandomizedSearchCV(agglomerative, param_grid_agglomerative, n_iter=20, scoring=dbs_scorer, cv=5, random_state=42)
random_search_agglomerative.fit(data_encoded)

print("Mejores parámetros (Agglomerative Random Search):", random_search_agglomerative.best_params_)
print("Mejor puntuación (Agglomerative Random Search):", -random_search_agglomerative.best_score_)  # Invertir la puntuación

"""## **K-Modes 2.0**"""

dataset1.head()

from os import initgroups
import seaborn as sns
from kmodes.kmodes import KModes

# Contruccion del modelo con los parametros seleccionados
k = 3  # Número de clusters
init = 'Huang'  # Método de inicialización
n_init = 5  # Número de veces que se ejecutará el algoritmo

km = KModes(n_clusters=k, init=init, n_init=n_init)
labels = km.fit_predict(dataset1)

# Se preserva el conjunto de datos original para evitar problemas
df2_kmeans_k3_huang = dataset1.copy()

# Se agregan las labels generadas a traves de kmodes
df2_kmeans_k3_huang['cluster'] = labels
df2_kmeans_k3_huang['cluster'].value_counts()

"""VISUALIZACION"""

from prince import MCA
import matplotlib.pyplot as plt
import prince
import seaborn as sns
import pandas as pd # Importa pandas explicitly


# Aplicar MCA
mca = MCA(n_components=2)
mca_results = mca.fit_transform(df2_kmeans_k3_huang.drop('cluster', axis=1))

# Crear un DataFrame para los resultados de MCA
mca_df = pd.DataFrame(data=mca_results.to_numpy(), columns=['Dim1', 'Dim2'], index=df2_kmeans_k3_huang.index)
mca_df['cluster'] = df2_kmeans_k3_huang['cluster']

# Visualizar los resultados
plt.figure(figsize=(10, 6))
sns.scatterplot(data=mca_df, x='Dim1', y='Dim2', hue='cluster', palette='viridis')
plt.title('Visualización del Clustering K-Modes(k=3, init=Huang) usando MCA')
plt.xlabel('Dimensión 1')
plt.ylabel('Dimensión 2')
plt.legend(title='Cluster')
plt.show()

from sklearn.metrics import silhouette_score
from sklearn.metrics import pairwise_distances

#Codificación de Variables Categóricas para Davies-Bouldin-Score
# Extract features and labels
X = df2_kmeans_k3_huang.drop('cluster', axis=1)
labels = df2_kmeans_k3_huang['cluster']

encoder = OneHotEncoder(sparse_output=False)
X_encoded = encoder.fit_transform(X)


#Calcular el Davies-Bouldin-Score
davies_bouldin = davies_bouldin_score(X_encoded, labels)
print(f"Davies-Bouldin Index: {davies_bouldin}")

# Calcular las distancias entre todos los puntos utilizando distancia Hamming
distancias = pairwise_distances( X_encoded, metric='hamming')

# Calcular el Silhouette Score
silhouette_avg = silhouette_score(distancias,labels, metric="precomputed")
print(f"Silhouette Score promedio para clustering: {silhouette_avg}")

# Representacion grafica por cada atributo segun el cluster al que pertenecen

# df_kmeans_k3_huang
for column in df2_kmeans_k3_huang.columns:
  if column != 'cluster':
    plt.figure(figsize=(10, 6))
    sns.countplot(x=column, hue='cluster', data=df2_kmeans_k3_huang)
    plt.title(f'Distribución de {column} por Cluster')
    plt.xlabel(column)
    plt.ylabel('Frecuencia')
    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better visibility
    plt.show()